---
title: "Worksheet 14: Statistical Learning"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE, 
                      warning = TRUE,
                      message = FALSE,
                      fig.align = "center", 
                      R.options = list(max.print=50))

# Edit the file starting below

```

In this worksheet, we will discuss a basic example of statistical learning.

## 1. Set up

Let's first load `tidyverse` because we always need functions to manipulate our dataset:

```{r, message=FALSE}
# Load packages 
library(tidyverse)
```

It's been a while since we introduced this dataset but remember `mtcars`?

```{r}
# Take a quick look
head(mtcars)
```

Let's discuss how we could predict the value of fuel efficiency (`mpg`) of the Alfa Romeo Alfetta GTV 2.0 based on other features of cars. Note: this particular car is not in the `mtcars` dataset but it was introduced in 1974 like many of the cars in this dataset.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Alfa_Romeo_Alfetta_Genf_%28despeck_col_balance_etc%29.jpg/1920px-Alfa_Romeo_Alfetta_Genf_%28despeck_col_balance_etc%29.jpg){width="403"}

## 2. Defining a model

If the goal is to predict the fuel efficiency of the car, the `mpg` variable is the **outcome**. But what should our predictor(s) be?

### a. A model with no predictor

How could we predict a value of `mpg` if we have no information about the Alfa Romeo Alfetta GTV 2.0? Consider the distribution of `mpg` for other cars and an average value:

```{r}
# Take a look at the values of mpg
mtcars |> 
    ggplot(aes(x = mpg)) +
    geom_histogram(binwidth = 2.5, center = 1.25, 
                   color = "black", fill = "blue") + 
    labs(x = "Miles per gallon",
         y = "Number of cars") 

# Find the mean value of mpg
mean(mtcars$mpg)
```

We could estimate the `mpg` of the Alfa Romeo Alfetta GTV 2.0 to be average, about 20 mpg.

### b. Some models with one predictor

Let's find the correlation coefficient between `mpg` and each predictor in the dataset:

```{r}
# Find the correlation matrix
cor(mtcars, use = "pairwise.complete.obs")[,1] # only show the first column
```

Some features of the cars seem to be linearly related to `mpg`. Let's focus on the weight of a car: the `wt` variable is the **predictor**. We can represent the relationship between the two variables with a scatterplot (by convention, the outcome variable is represented on the y-axis). Before, we estimated the predicted `mpg` to just be the average, regardless of the weight (a very simple model):

```{r}
# Represent mpg vs. weight
mtcars |> 
  ggplot(aes(x = wt, y = mpg)) + 
  geom_point(size = 4) + 
  labs(x = "Weight (thousands of lbs)",
       y = "Miles per gallon",
       title = "Using the mean: Prediction is constant") +
  # Show the overall mean of mpg as a horizontal line
  geom_hline(aes(yintercept = mean(mpg)), size = 3, color = "steelblue")
```

With this model, regardless of the weight, we predict the fuel efficiency to be about 20 mpg.

We know that the weight of an Alfa Romeo Alfetta GTV 2.0 was about 2500 lbs (see [Wikipedia](https://en.wikipedia.org/wiki/Alfa_Romeo_Alfetta#)). Consider the following model that goes through each data point:

```{r}
mtcars |>
  ggplot(aes(x = wt, y = mpg)) +
  # Consider a very refined model that goes through each data point
  geom_line(color = "steelblue", size = 2) +
  geom_point(size = 4) + 
  labs(x = "Weight (thousands of lbs)",
       y = "Miles per gallon",
       title = "Using all data points: Prediction is car specific") +
  # Show the weight as a vertical line
  geom_vline(xintercept = 2.5, color = "red")
```

Has the prediction for the `mpg` of the Alfa Romeo Alfetta GTV 2.0 changed based on the value of its weight?

**Yes, based on this plot now we might predict that the `mpg` of the Alfa Romeo Alfetta GTV 2.0 is around 22 mpg**

Now, let's consider another model with an overall linear trend (linear regression which we will talk about in the next worksheet):

```{r}
mtcars |>
  ggplot(aes(x = wt, y = mpg)) +
  # Consider a linear regression model
  geom_smooth(method = "lm", se = FALSE, color = "steelblue", size = 2) +
  geom_point(size = 4) + 
  labs(x = "Weight (thousands of lbs)",
       y = "Miles per gallon",
       title = "Using linear regression: Prediction is based on a trend") +
  # Show the weight as a vertical line
  geom_vline(xintercept = 2.5, color = "red")
```

Has the prediction for the `mpg` of the Alfa Romeo Alfetta GTV 2.0 changed based on this new model?

**Yes, based on this new model (linear regression) we might predict that the Alfa Romeo Alfetta GTV 2.0 has an mpg of 23-25 mpg**

Which model is more useful to make a prediction? It's hard to say. Overall, we want:

-   Predictions that are accurate (close to the truth)

-   Predictions that are stable (should not change much if we add new data)

But it is difficult to be both accurate and stable: there is a trade-off between these two conditions.

## 3. Comparing models

Let's add a new data point, about the Volvo 200 Series in 1974:

```{r}
mtcars |> 
  # Only keep variables of interest
  select(mpg, wt) |>
  # Add a new data point (add a new row)
  rbind(data.frame(mpg = 29,
                   wt = 2.5)) |>
  # Only look at the last 6 rows
  tail()
```

How does this new data point affect, or not, our models?

#### **Try it! Represent the 3 models (constant, all data points, linear regression) with the new data point. Which model resulted in the biggest change in the prediction for the Alfa Romeo Alfetta GTV 2.0? the smallest change? Note: only guess-estimate the prediction on the visualizations.**

```{r}
# Write and submit code here!

# constant model
mtcars |>
    select(mpg, wt) |>
    rbind(data.frame(mpg = 29, wt = 2.5)) |>
    ggplot(aes(x = wt, y = mpg)) + 
        geom_point(size = 4) + 
        labs(x = "Weight (thousands of lbs)",
             y = "Miles per gallon",
             title = "Using the mean: Prediction is constant") +
        # Show the overall mean of mpg as a horizontal line
        geom_hline(aes(yintercept = mean(mpg)), size = 3, color = "steelblue")

# all data points model
mtcars |>
    select(mpg, wt) |>
    rbind(data.frame(mpg = 29, wt = 2.5)) |>
    ggplot(aes(x = wt, y = mpg)) +
        # Consider a very refined model that goes through each data point
        geom_line(color = "steelblue", size = 2) +
        geom_point(size = 4) + 
        labs(x = "Weight (thousands of lbs)",
             y = "Miles per gallon",
             title = "Using all data points: Prediction is car specific") +
        # Show the weight as a vertical line
        geom_vline(xintercept = 2.5, color = "red")

# linear regression model
mtcars |>
    select(mpg, wt) |>
    rbind(data.frame(mpg = 29, wt = 2.5)) |>
    ggplot(aes(x = wt, y = mpg)) +
         # Consider a linear regression model
         geom_smooth(method = "lm", se = FALSE, color = "steelblue", size = 2) +
         geom_point(size = 4) + 
         labs(x = "Weight (thousands of lbs)",
              y = "Miles per gallon",
              title = "Using linear regression: Prediction is based on a trend") +
         # Show the weight as a vertical line
         geom_vline(xintercept = 2.5, color = "red")
```

**The smallest change is debatable between the constant model in which our prediction is based on the mean mpg of all the cars where the calculated mean slightly moved up and the linear regression model in which the regression line also slightly moved up. The biggest change was the all data points model since we placed a car at exactly at 2.5k pounds, our prediction now switches from around 22 mpg to 29 mpg.**

Notes on the bias-variance trade-off:

-   A model that is stable and not sensitive to changes in the data is said to have **low variance**. A model that is highly sensitive to changes in the data is said to have **high variance** (and is not very useful to make predictions for new data).

-   A model that is flexible and captures complex patterns in the data is said to have **low bias** (however, there is a risk of overfitting the data). A model that is simplistic and does not take into account the complexities in the data is said to have **high bias** (there is a risk of underfitting the data).

Ideally, a model should have low variance and low bias but can you see that there is a trade-off between the two conditions?

For each model we introduce this semester, we will introduce a measure to check potential *bias* by comparing our predictions to the real data (looking at what we call "residuals"). Then we will also check potential *variance* by comparing the model based on new data (this is called cross-validation).
