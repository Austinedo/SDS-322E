---
title: "Worksheet 20: Dimension Reduction"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

```{r global_options, include=FALSE}
# The following code is a basic setup of options for your document
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE, 
                      warning = TRUE,
                      message = FALSE,
                      fig.align = "center", 
                      R.options = list(max.print=50))

# Edit the file starting below

```

In this worksheet, we will introduce a technique to reduce the number of variables in our dataset.

## 1. Set up and introduction to the dataset

We will use the `tidyverse` package as usual but also `ade4` to access a built-in dataset, `ggcorrplot` to visualize a correlation matrix, and `factoextra` to provide information about the algorithm for dimension reduction.

```{r, eval=FALSE}
# Install new packages (only needed once!)
install.packages("ade4")
install.packages("ggcorrplot")
install.packages("factoextra")
```

```{r, message=FALSE}
# Load packages
library(tidyverse)
library(ade4)
library(ggcorrplot)
library(factoextra)
```

Let's consider the built-in database `olympic` which gives the performances of 33 men in the decathlon (10 disciplines) at the Olympic Games in 1988. We will focus on the dataset `tab`.

```{r}
# Save the database into your environment, then the dataset
data("olympic")
athletes <- olympic$tab

# Take a quick look at the dataset
head(athletes)
```

The names of the variables might not be very intuitive so let's rename them:

```{r}
# Quick cleanup
athletes <- athletes |>
  # Translate the variable names (from French!)
  rename(disc = disq, weight = poid, high_jump = haut,
  # Make the names more explicit
         long_jump = long, javelin = jave, perch = perc,
  # Add indication of distance (R does not like digits for variable names)
         dist_100 = `100`, dist_110 = `110`,
         dist_400 = `400`, dist_1500 = `1500`)
```

We will investigate the information brought by these 10 variables.

## 2. Correlation

Correlation describes the (linear) relationship between two variables. For example, let's look at the relationship between time to run 100 meters and length of a long jump:

```{r}
# Visualize the relationship between dist_100 and long_jump
ggplot(athletes, aes(x = dist_100, y = long_jump)) +
  geom_point() + geom_smooth(method = "lm", se =  F) +
  labs(x = "Time to run 100 meters (in seconds)",
       y = "Distance for long jump (in meters)")
```

We can use the correlation coefficient to describe the strength and direction of the relationship between those two variables:

```{r}
# Find the correlation between two variables
cor(athletes$dist_100, athletes$long_jump, 
    use = "pairwise.complete.obs") # ignore missing values
```

What if we would like to find the correlation coefficients between all pairs of numeric variables? That's a lot of calculations of the correlation coefficients...

```{r}
# Find pairwise correlations
cor(athletes, use = "pairwise.complete.obs")
```

The output is a matrix representing correlations so it is called a correlation matrix! It is pretty ugly though... let's make it pretty with `ggcorrplot(correlation_matrix)`!

```{r}
# Use the ggcorrplot to visualize the correlation matrix
ggcorrplot(cor(athletes))
```

We can add some options to make the correlation matrix even prettier:

```{r}
# We can add some options
ggcorrplot(cor(athletes),
           type = "upper", # upper diagonal
           lab = TRUE, # print values
           method = "circle") # use circles with different sizes
```

It is now easier to spot the variables that are the most correlated.

#### **Try it! Create a graph to display the relationship between the pair of variables that has the highest positive correlation coefficient. Describe the relationship.**

```{r}
# Write and submit code here!
athletes |>
    ggplot(aes(x = weight, y = disc)) +
        geom_point() +
        geom_smooth(method = 'lm', se = FALSE) +
        labs(title = 'Relationship between shotput and discus throw distance',
             x = 'Shotput',
             y = 'Discus throw distance')
```

**The relationship between `weight` and `disc` is positively correlated where as `weight` increase, `disc` also increases.**

## 3. Principal Component Analysis

The 4 steps in PCA are to:

1.  Prepare the data: Always center (subtract the mean from each variable), usually scale (also divide by the standard deviation).

2.  Perform PCA: Using `prcomp()` on your prepared variables.

3.  Choose the number of principal components: Make a scree plot (or choose based on variance or interpretability).

4.  Consider PC scores (the new coordinates for each observation on PCs of interest) and visualize and interpret (if possible) retained PCs and scores.

### a. Prepare the dataset and explore correlations

We would like to group variables that give similar information. It is a good practice to scale our variables so they are all in the same unit (how many standard deviations away a value is from the mean) with `scale()`

```{r}
# Prepare the dataset
athletes_scaled <- athletes |> 
  # Scale the variables
  scale() |>
  # Save as a data frame
  as.data.frame()

# Take a look at the scaled data
head(athletes_scaled)
```

What does a negative value indicate in the scaled data? What does a positive value indicate?

**It means that the value was below the average for that variable and a positive value indicates**

### b. Perform PCA

Let's perform PCA on our 10 variables using `prcomp()`.

```{r}
# PCA performed with the function prcomp()
pca <- athletes_scaled |>
  prcomp()

# The output creates 5 different objects
names(pca)
```

Without going into too much detail, let's describe the element `x`. Instead of having the performances of the 33 athletes for each 10 disciplines, we have new values according to the new variables PC1, PC2, ..., PC10. The first few principal components (PC), also called dimensions, try to maximize the variation explain.

```{r}
# New perspective on our data
pca$x |> as.data.frame()
```

Let's use the new dimensions (PC1 and PC2) to represent the athletes:

```{r}
# Visualize the individuals according to PC1 and PC2
fviz_pca_ind(pca, 
             repel = TRUE) # Avoid text overlapping for the row number
```

*Note that the numbers shown on the scatterplot represent the row of each athlete.*

The scatterplot above is a new perspective on our data: it shows how the 33 athletes compare to each other, taking into account the 10 disciplines which are summarized with Dim1 and Dim2, the first two principal components. Since we reduced the amount of variables, we lost some information about how the 33 athletes vary from each other: Dim1 takes into account 34.2% of the total variation and Dim2 takes into account another 26.1% of the total variation.

### c. Choose the number of principal components

The idea is to reduce the number of variables so we would like to keep only a few of the principal components (also called dimensions). A scree plot displays the amount of variance explained by each principal component. The more we explain the total variation, the better!

```{r}
# Visualize percentage of variance explained for each PC in a scree plot
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50))
```

We are usually looking to keep about 80% of the variance with the few first principal components. Here keeping the first 4 components will add up to about 78.5%.

### d. Interpret retained PCs and scores

Each PC is actually a linear combination of the old variables (each of the 10 disciplines). We can take a look at the contribution of each variable to each component:

```{r}
# Visualize the contributions of the variables to the PCs in a table
get_pca_var(pca)$coord |> as.data.frame()
```

For example, the first principal component (`Dim.1`) is:

$$
Dim.1 = -0.7689031 * dist\_100 + 0.7285412 * long\_jump + ... -0.3145678 * dist\_1500
$$

#### **Try it! Use `dplyr` functions to find the variable that contributes the most positively to the first principal component and the variable that contributes the most negatively as well.**

```{r}
# Write and submit code here!
get_pca_var(pca)$coord |>
    as.data.frame() |>
    arrange(desc(Dim.1))
```

**The variable that contributes the most positively to the first principle component `Dim.1` is `long_jump` and the variable that contributes the most negatively is `dist_110`**

We can visualize the contributions of the variables with what we call a correlation circle:

```{r}
# Correlation circle
fviz_pca_var(pca, col.var = "black", 
             repel = TRUE) # Avoid text overlapping of the variable names
```

Based on this visualization, we can see that some disciplines contribute positively to the first component and some contribute negatively to that same dimension. What do you notice when comparing the nature of those disciplines opposing each other on the first dimension?

**We see that for the first dimension, variables involving events that were timed events negatively contributed to the first dimension while variables involving events not involving time positively contributed to the first dimension**

Finally, we can visualize both the individuals and the variables' contributions in a single plot called a biplot:

```{r}
# Visualize both variables and individuals in the same graph
fviz_pca_biplot(pca, 
             repel = TRUE) # Avoid text overlapping of the names
```

The labels for the athletes show their overall decathlon rank. What do you notice about were the best ranked athletes are located? the worst ranked athletes?

**The best ranked athletes are located on the right side or the side with the events positively contribute to the first principle component while the worst ranked athletes are located on the side with the events that negatively contribute to the first principle component**

What does it mean for an athlete to have a high value for the first dimension?

**An athlete having a high value in the first dimension means they performed well (above average) in variables/events that positively contributed to the first principle component/first dimension**
